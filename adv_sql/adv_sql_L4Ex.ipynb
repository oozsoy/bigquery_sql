{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# We will import a customized function called client which actually returns an authorized bigquery client object with right credentials\n",
    "# this will cost us an extra pair of () each time we call the client object which is now called by the function client we define in bq_sa_auth.py \n",
    "\n",
    "from bq_sa_auth import client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 4: Writing Efficient Queries**\n",
    "\n",
    "Write queries to run faster and use less data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refer to the kaggle [tutorial](https://www.kaggle.com/code/alexisbcook/writing-efficient-queries/tutorial) for an Introduction to the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it doesn't matter whether your query is efficient or not. For example, you might write a query you expect to run only once, and it might be working on a small dataset. In this case, anything that gives you the answer you need will do.\n",
    "\n",
    "But what about queries that will be run many times, like a query that feeds data to a website? Those need to be efficient so you don't leave users waiting for your website to load.\n",
    "\n",
    "Or what about queries on huge datasets? These can be slow and cost a business a lot of money if they are written poorly.\n",
    "\n",
    "Most database systems have a query optimizer that attempts to interpret/execute your query in the most effective way possible. But several strategies can still yield huge savings in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some useful functions:**\n",
    "\n",
    "We will use two functions to compare the efficiency of different queries:\n",
    "\n",
    "- `show_amount_of_data_scanned()` shows the amount of data the query uses.\n",
    "- `show_time_to_run()` prints how long it takes for the query to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def show_amount_of_data_scanned(query):\n",
    "    # dry_run lets us see how much data the query uses without running it\n",
    "    dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "    query_job = client.query(query, job_config=dry_run_config)\n",
    "    print(f\"Data processed: {round(query_job.total_bytes_processed / 10**9, 3)} GB\")\n",
    "    \n",
    "def show_time_to_run(query):\n",
    "    time_config = bigquery.QueryJobConfig(use_query_cache=False)\n",
    "    start = time()\n",
    "    query_result = client.query(query, job_config=time_config).result()\n",
    "    end = time()\n",
    "    print(f\"Time to run: {round(end-start, 3)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategies**\n",
    "1) **Only select the columns you want.**\n",
    "\n",
    "It is tempting to start queries with SELECT * FROM .... It's convenient because you don't need to think about which columns you need. But it can be very inefficient.\n",
    "\n",
    "This is especially important if there are text fields that you don't need, because text fields tend to be larger than other fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed: 2682.118 GB\n",
      "Data processed: 2.531 GB\n"
     ]
    }
   ],
   "source": [
    "star_query = \"SELECT * FROM `bigquery-public-data.github_repos.contents`\"\n",
    "show_amount_of_data_scanned(star_query)\n",
    "\n",
    "basic_query = \"SELECT size, binary FROM `bigquery-public-data.github_repos.contents`\"\n",
    "show_amount_of_data_scanned(basic_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see a 1000X reduction in data being scanned to complete the query, because the raw data contained a text field that was 1000X larger than the fields we might need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Read less data.**\n",
    "\n",
    "Both queries below calculate the average duration (in seconds) of one-way bike trips in the city of San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed: 0.076 GB\n",
      "Data processed: 0.06 GB\n"
     ]
    }
   ],
   "source": [
    "more_data_query = \"\"\"\n",
    "                  SELECT MIN(start_station_name) AS start_station_name,\n",
    "                      MIN(end_station_name) AS end_station_name,\n",
    "                      AVG(duration_sec) AS avg_duration_sec\n",
    "                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                  WHERE start_station_id != end_station_id \n",
    "                  GROUP BY start_station_id, end_station_id\n",
    "                  LIMIT 10\n",
    "                  \"\"\"\n",
    "show_amount_of_data_scanned(more_data_query)\n",
    "\n",
    "less_data_query = \"\"\"\n",
    "                  SELECT start_station_name,\n",
    "                      end_station_name,\n",
    "                      AVG(duration_sec) AS avg_duration_sec                  \n",
    "                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                  WHERE start_station_name != end_station_name\n",
    "                  GROUP BY start_station_name, end_station_name\n",
    "                  LIMIT 10\n",
    "                  \"\"\"\n",
    "show_amount_of_data_scanned(less_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a 1:1 relationship between the station ID and the station name, we don't need to use the `start_station_id` and `end_station_id` columns in the query. By using only the columns with the station IDs, we scan less data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Avoid N:N JOINs.**\n",
    "\n",
    "Most of the JOINs that you have executed in this course have been 1:1 JOINs. In this case, each row in each table has at most one match in the other table.\n",
    "\n",
    "Another type of JOIN is an N:1 JOIN. Here, each row in one table matches potentially many rows in the other table.\n",
    "\n",
    "Finally, an N:N JOIN is one where a group of rows in one table can match a group of rows in the other table. Note that in general, all other things equal, this type of JOIN produces a table with many more rows than either of the two (original) tables that are being JOINed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run: 13.964 seconds\n",
      "Time to run: 3.071 seconds\n"
     ]
    }
   ],
   "source": [
    "big_join_query = \"\"\"\n",
    "                 SELECT repo,\n",
    "                     COUNT(DISTINCT c.committer.name) as num_committers,\n",
    "                     COUNT(DISTINCT f.id) AS num_files\n",
    "                 FROM `bigquery-public-data.github_repos.commits` AS c,\n",
    "                     UNNEST(c.repo_name) AS repo\n",
    "                 INNER JOIN `bigquery-public-data.github_repos.files` AS f\n",
    "                     ON f.repo_name = repo\n",
    "                 WHERE f.repo_name IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                 GROUP BY repo\n",
    "                 ORDER BY repo\n",
    "                 \"\"\"\n",
    "show_time_to_run(big_join_query)\n",
    "\n",
    "small_join_query = \"\"\"\n",
    "                   WITH commits AS\n",
    "                   (\n",
    "                   SELECT COUNT(DISTINCT committer.name) AS num_committers, repo\n",
    "                   FROM `bigquery-public-data.github_repos.commits`,\n",
    "                       UNNEST(repo_name) as repo\n",
    "                   WHERE repo IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                   GROUP BY repo\n",
    "                   ),\n",
    "                   files AS \n",
    "                   (\n",
    "                   SELECT COUNT(DISTINCT id) AS num_files, repo_name as repo\n",
    "                   FROM `bigquery-public-data.github_repos.files`\n",
    "                   WHERE repo_name IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                   GROUP BY repo\n",
    "                   )\n",
    "                   SELECT commits.repo, commits.num_committers, files.num_files\n",
    "                   FROM commits \n",
    "                   INNER JOIN files\n",
    "                       ON commits.repo = files.repo\n",
    "                   ORDER BY repo\n",
    "                   \"\"\"\n",
    "\n",
    "show_time_to_run(small_join_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first query has a large N:N JOIN. By rewriting the query to decrease the size of the JOIN, we see it runs much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### 1) You work for Pet Costumes International.\n",
    "\n",
    "You need to write three queries this afternoon. You have enough time to write working versions of all three, but only enough time to think about optimizing one of them.  Which of these queries is most worth optimizing?\n",
    "\n",
    "1. A software engineer wrote an app for the shipping department, to see what items need to be shipped and which aisle of the warehouse to go to for those items. She wants you to write the query. It will involve data that is stored in an `orders` table, a `shipments` table and a `warehouseLocation` table. The employees in the shipping department will pull up this app on a tablet, hit refresh, and your query results will be shown in a nice interface so they can see what costumes to send where.\n",
    "\n",
    "\n",
    "2. The CEO wants a list of all customer reviews and complaints… which are conveniently stored in a single `reviews` table. Some of the reviews are really long… because people love your pirate costumes for parrots, and they can’t stop writing about how cute they are.\n",
    "\n",
    "\n",
    "3. Dog owners are getting more protective than ever. So your engineering department has made costumes with embedded GPS trackers and wireless communication devices. They send the costumes’ coordinates to your database once a second. You then have a website where owners can find the location of their dogs (or at least the costumes they have for those dogs). For this service to work, you need a query that shows the most recent location for all costumes owned by a given human. This will involve data in a `CostumeLocations` table as well as a `CostumeOwners` table.\n",
    "\n",
    "So, which of these could benefit most from being written efficiently?  Set the value of the `query_to_optimize` variable below to one of `1`, `2`, or `3`.  (Your answer should have type **integer**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: 3rd query would benefit most an optimization procedure. This is because the data on the pets location which is tracked every second will be very large and queries run on this data, being likely to be recurrent will need the optimizated more as compared to the other cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Make it easier to find Mitzie! \n",
    "\n",
    "You have the following two tables:\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/E9jikOQ.png)\n",
    "\n",
    "The `CostumeLocations` table shows timestamped GPS data for all of the pet costumes in the database, where `CostumeID` is a unique identifier for each costume.  \n",
    "\n",
    "The `CostumeOwners` table shows who owns each costume, where the `OwnerID` column contains unique identifiers for each (human) owner.  Note that each owner can have more than one costume!  And, each costume can have more than one owner: this allows multiple individuals from the same household (all with their own, unique `OwnerID`) to access the locations of their pets' costumes.\n",
    "\n",
    "Say you need to use these tables to get the current location of one pet in particular: Mitzie the Dog recently ran off chasing a squirrel, but thankfully she was last seen in her hot dog costume!\n",
    "\n",
    "One of Mitzie's owners (with owner ID `MitzieOwnerID`) logs into your website to pull the last locations of every costume in his possession.  Currently, you get this information by running the following query:\n",
    "\n",
    "```sql\n",
    "WITH LocationsAndOwners AS \n",
    "(\n",
    "SELECT * \n",
    "FROM CostumeOwners co INNER JOIN CostumeLocations cl\n",
    "   ON co.CostumeID = cl.CostumeID\n",
    "),\n",
    "LastSeen AS\n",
    "(\n",
    "SELECT CostumeID, MAX(Timestamp)\n",
    "FROM LocationsAndOwners\n",
    "GROUP BY CostumeID\n",
    ")\n",
    "SELECT lo.CostumeID, Location \n",
    "FROM LocationsAndOwners lo INNER JOIN LastSeen ls \n",
    "\tON lo.Timestamp = ls.Timestamp AND lo.CostumeID = ls.CostumeID\n",
    "WHERE OwnerID = MitzieOwnerID\n",
    "```\n",
    "\n",
    "Is there a way to make this faster or cheaper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER: Since each costume is owned by multiple owners and each owner can have more than one costume. The first INNER JOIN at the top of the query above will result with a large table (where all columns are selected) and will render it inefficient. Instead of performing a large merge initially and filtering the data on this large table, we can try to filter first OWNERID's using MITZIEOWNERID and work out the filtering on a much reduced table. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
